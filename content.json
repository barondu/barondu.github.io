[{"title":"笔记 OSC_Processes3","date":"2017-01-08T23:27:27.000Z","path":"2017/01/08/Processes 3/","text":"Processes 3 Overview1 Threads vs. processes2 Different thread implementations3 POSIX Threads (PThreads) ThreadsThreads from an OS Perspective[观点] A process consists of two fundamental units Resources: all related resources are grouped together A logical address space containing the process image (program, data, heap, stack) Files, I/O devices, I/O channels, . . . Execution trace[执行追踪], i.e., an entity that gets executed A process can share its resources between multiple execution traces, i.e., multiple threads running in the same resource environment Every thread has its own execution context (e.g. program counter, stack, registers) All threads have access to the process’ shared resources E.g. files, one thread opens a file, all threads of the same process can access the file Global variables, memory, etc. (⇒ synchronisation!) Some CPUs (hyperthreaded ones) have direct hardware support for multi-threading Similar to processes, threads have: States and transitions (new, running, blocked, ready, terminated) A thread control blockThreads create/terminate/switch with less overhead (address space remains the same for threads of the same process) Inter-thread communication is easier/faster than inter-process communication (threads share memory by default) No protection boundaries[边界] are required in the address space (threads are cooperating, belong to the same user, and have a common goal) Synchronisation has to be considered carefully! Why Use Threads Multiple related activities apply to the same resources, these resources should be accessible/shared Processes will often contain multiple blocking tasks I/O operations (thread blocks, interrupt marks completion) Memory access: pages faults are result in blocking Such activities should be carried out in parallel/concurrently Application examples: webservers, make program, spreadsheets, word processors, processing large data volumes ####OS Implementations of Threads User threads Kernel threads Hybrid[混合] implementations User ThreadsMany-to-One Thread management (creating, destroying, scheduling, thread control block manipulation[处理]) is carried out in user space with the help of a user library The process maintains a thread table managed by the runtime system without the kernel’s knowledge Similar to process table Used for thread switching Tracks thread related information Advantages: Threads are in user space (i.e., no mode switches required) Full control over the thread scheduler OS independent[独立不受约束] (threads can run on OS that do not support them) Disadvantages: Blocking system calls suspend[延缓] the entire process (user threads are mapped onto a single process, managed by the kernel) No true parallelism (a process is scheduled on a single CPU) Clock interrupts are non-existent (i.e. user threads are non-preemptive) Page faults[错误] result in blocking the process Kernel ThreadsOne-to-One The kernel manages the threads, user application accesses threading facilities[工具] through API and system calls Thread table is in the kernel, containing thread control blocks (subset of process control blocks) If a thread blocks, the kernel chooses thread from same or different process (↔ user threads) Windows and Linux apply this approach Advantages: True parallelism can be achieved No run-time system needed Disadvantage: Frequent mode switches take place, resulting in lower performance Frequent mode switches take place, resulting in lower performance Performance Hybrid ImplementationsMany-to-Many User threads are multiplexed onto kernel threads Kernel sees and schedules the kernel threads (a limited number) User application sees user threads and creates/schedules these (an “unrestricted” number) Comparison Thread Management Thread libraries provide an API/interface for managing threads (e.g. creating, running, destroying, synchronising, etc.) Thread libraries can be implemented: Entirely in user space (i.e. user threads) Based on system calls Examples of thread APIs include POSIX’s PThreads, Windows Threads, and Java Threads The PThread specification can be implemented as user or kernel threads POSIX threads are a specification that “anyone” can implement, i.e., it defines a set of APIs (function calls, over 60 of them) and what they do Summary Threads vs. processes Thread implementations (user, kernel and hybrid) PThreads","tags":[{"name":"笔记","slug":"笔记","permalink":"http://barondu.com/tags/笔记/"},{"name":"OSC","slug":"OSC","permalink":"http://barondu.com/tags/OSC/"},{"name":"processes","slug":"processes","permalink":"http://barondu.com/tags/processes/"}]},{"title":"笔记 OSC_Processes2","date":"2017-01-08T23:26:14.000Z","path":"2017/01/08/Processes 2/","text":"Processes 2 Overview Introduction to process scheduling Types of process schedulers Evaluation criteria for scheduling algorithms Typical process scheduling algorithms Process SchedulingContext The OS is responsible for managing and scheduling processes Decide when to admit processes to the system (new → ready) Decide which process to run next (ready → run) Decide when and which processes to interrupt (running → ready) It relies on the scheduler (dispatcher) to decide which process to run next, which uses a scheduling algorithm to do so The type of algorithm used by the scheduler is influenced by the type of operating system (e.g., real time vs. batch) Classification by Time Horizon Long term: applies to new processes and controls the degree of multiprogramming by deciding which processes to admit to the system A good mix of CPU and I/O bound processes is favourable to keep all resources as busy as possible Usually absent in popular modern OS Medium term: controls swapping and the degree of multi-programming Short term: decide which process to run next Usually called in response to clock interrupts, I/O interrupts, or blocking system calls Invoked[调用] very frequently, hence must be fast Manages the ready queue Classification by Approach Non-preemptive: processes are only interrupted voluntarily[自愿的] (e.g., I/O operation or “nice” system call – yield()) Preemptive[优先的]: processes can be interrupted forcefully or voluntarily This requires context switches which generate overhead, too many of them should be avoidedPrevents processes from monopolising[垄断的] the CPU Most popular modern operating systems are preemptive Performance Assessment[性能评估]User oriented[导向]criteria: Response time: minimise the time between creating the job and its first execution Turnaround time: minimise the time between creating the job and finishing it Predictability[可调度性]: minimise the variance[差异，方差] in processing times System oriented criteria: Throughput[吞吐量]: maximise the number of jobs processed per hour Fairness[公平性]: Are processing power/waiting time equally distributed? Are some processes kept waiting excessively long (starvation) Evaluation criteria can be conflicting, i.e., reducing the response time may increase context switches and may worsen the throughput and increase the turn around time Scheduling AlgorithmsAlgorithms considered: First Come First Served (FCFS)/ First In First Out (FIFO) Shortest job first Round Robin Priority queues Performance measures used: Average response time Average turnaround time First Come First Served Concept: a non-preemtive algorithm that operates as a strict[严格的] queueing mechanism[机制] and schedules the processes in the same order that they were added to the queue Advantages: positional fairness and easy to implement Disadvantages: good for long processes over short ones Could compromise[危害] resource utilisation, i.e., CPU vs. I/O devices Shortest Job First Concept: A non-preemtive algorithm that starts processes in order of ascending[递增] processing time using a provided/known estimate of the processing Advantages: always result in the good turn around time Disadvantages: Starvation[饿死] might occur Fairness and predictability are compromised Processing times have to be known beforehand Round Robin Concept: a preemptive version of FCFS that forces context switches at periodic[周期性] intervals[间隔] or time slices Processes run in the order that they were added to the queue Processes are forcefully interrupted by the timerAdvantages: Improved response time Effective for general purpose time sharing systemsDisadvantages: Increased context switching and thus overhead Favours CPU bound processes (which usually run long) over I/O processes (which do not run long) Can reduce to FCFS The length of the time slice must be carefully considered! a low response time is achieved with a small time slice (e.g. 1ms) ⇒ low throughput a high throughput is achieved with a large time slice (e.g. 1000ms) ⇒ high response time If a time slice is only used partially, the next process starts immediately Priority Queues Concept: A preemptive algorithm that schedules processes by priority (high → low) The process priority is saved in the process control block Advantages: can prioritise[优先] I/O bound jobs Disadvantages: low priority processes may suffer from starvation (with static priorities) ##Summary Types of schedulers: preemptive/non-preemptive, long/medium/short term) Performance evaluation criteria Scheduling algorithms: FCFS, SJF, Round Robin, Priority Queues","tags":[{"name":"笔记","slug":"笔记","permalink":"http://barondu.com/tags/笔记/"},{"name":"OSC","slug":"OSC","permalink":"http://barondu.com/tags/OSC/"},{"name":"processes","slug":"processes","permalink":"http://barondu.com/tags/processes/"}]},{"title":"笔记 OSC_Processes1","date":"2017-01-06T11:42:14.000Z","path":"2017/01/06/Processes1/","text":"Processes 1 Overview Introduction to processes and their implementation Process states and state transitions System calls for process management Processes and Implementation Definition: “a process is a running instance of a program”进程是程序的运行实例 A process is registered with the OS using its “control structures”: i.e. an entry in the OS’s process table to a process control blocks (PCB) The process control block contains all information necessary to manage the process and is necessary for context switching in multi-programmed systems A process’ memory image contains: The program code (could be shared between multiple processes running the same code) A data segment, stack and heap Every process has its own logical address space, in which the stack and heap are placed at opposite[相反的] sides to allow them to grow Process States and TransitionsSates: A new process has just been created (has a PCB) and is waiting to be admitted (it may not yet be in memory) A ready process is waiting for CPU to become available (e.g. unblocked or timer interrupt) A running process “owns” the CPU A blocked process cannot continue, e.g. is waiting for I/O A terminated process is no longer executable (the data structures - PCB - may be temporarily preserved) A suspended[废除的] process is swapped out[换出] (not discussed further) Transitions： New -&gt; ready: admit the process and commit to execution Running -&gt; blocked: e.g. process is waiting for input or carried out asystem call Ready -&gt; running: the process is selected by the process scheduler Blocked -&gt; ready: event happens, e.g. I/O operation has finished . Running -&gt; ready: the process is preempted, e.g., by a timer interrupt orby pause Running -&gt;strong text exit: process has finished, e.g. program ended or exceptionencountered The interrupts/traps/system calls lie on the basis of the transitions Context SwitchingMulti-programming Modern computers are multi-programming systems Assuming a single processor system, the instructions of individual processes are executed sequentially Multi-programming goes back to the “MULTICS” age Multi-programming is achieved by alternating[交替]processes and context switching True parallelism requires multiple processors并不是真正的multi-programming When a context switch takes place, the system saves the state of the old process and loads the state of the new process (creates overhead) Saved -&gt; the process control block is updated (Re-)started -&gt; the process control block read A trade-off[权衡] exists between the length of the time-slice and the context switch time Short time slices result in good response times but low effective “utilisation”[使用] e.g.: 99*(1+1)=198ms Long time slices result in poor response times but better effective “utilisation” e.g.: 99 * (100 + 1) = 9999ms A process control block contains three types of attributes: Process identification (PID, UID, Parent PID) Process control information (process state, scheduling information, etc.) Process state information (user registers, program counter, stack pointer, program status word, memory management information, files, etc.) Process control blocks are kernel data structures, i.e. they are protected and only accessible in kernel mode! Allowing user applications to access them directly could compromise[威胁] their integrity[完整性] The operating system manages them on the user’s behalf through system calls (e.g. to set process priority) Tables and Control Blocks An operating system maintains information about the status of “resources” in tables Process tables (process control blocks) Memory tables (memory allocation, memory protection, virtual memory) I/O tables (availability, status, transfer information) File tables (location, status) The process table holds a process control block for each process, allocated upon process creation Tables are maintained by the kernel and are usually cross referencedSwitching Processes Save process state (program counter, registers) Update PCB (running -&gt; ready/blocked) Move PCB to appropriate queue (ready/blocked) Run scheduler, select new process Update to running state in the new PCB Update memory management unit (MMU) Restore process System Calls System calls are necessary to notify the OS that the process has terminated Resources must be de-allocated Output must be flushed Process admin may have to be carried out A system calls for process termination: UNIX/Linux: exit(), kill() Windows: TerminateProcess() Summary Definition of a process and their implementation in operating systems States, state transitions of processes Kernel structures for processes and process management System calls for process management","tags":[{"name":"笔记","slug":"笔记","permalink":"http://barondu.com/tags/笔记/"},{"name":"OSC","slug":"OSC","permalink":"http://barondu.com/tags/OSC/"},{"name":"processes","slug":"processes","permalink":"http://barondu.com/tags/processes/"}]},{"title":"笔记 OSC_Introduction2","date":"2017-01-03T23:50:14.000Z","path":"2017/01/03/Introduction2/","text":"Introduction 2 Overview CPU design Address spaces, interruts OS structures/implementation CPU design A CPU basic cycle consist of fetch[取], decode, execute Every CPU has his own instruction set CPU has a set of registers Registers are used to store data and for special functions (e.g. program counter, program status word – mode bit) The compiler/programmer decides what to keep in the registers Context switching[上下文切换] must save and restore the CPU’s internal state, including its registers Memory Management Unit (MMU) Memory adsresses from 0 to MAX Variables are mnemonic[帮助记忆的] names for memory addresses You don’t know where the process will run in physical memory at compile time Multiple processes run on modern machines The compiler assumes that it will start running at 0 (logical address space) An offset[补偿] is added at runtime by the MMU (physical address space) physical address = logical address + offset Modern computer use a logical and physical memory addresses: Every process has a logical address space – [0,MAX64] (theoretically理论上) The machine has a physical address space – [0, MAX ] (MAX determined by the amount of physical memory) Address translation takes place in MMU physical = f (logical ) A context switch between processes invalidates[使无效] the MMU (as well as registers, cache, … ) Timer interrupts Interrupts temporarily pause a process’s normal operation Different types of interrupts: Timer interrupts by CPU clock I/O interrupts for I/O completion or error codes Software generated, e.g. errors and exceptions Timer generates an interrupt CPU finishes current instruction and tests for interrupt Transfer to interrupt service routine Hardware saves current process state (PSW, program counter) Set program counter to interrupt service routine Save registers and other state information Carry out[执行] interrupt service routine (scheduler) Restore next process to run Moore’s “law” Moore’s “law”: “The number of transistors on an integrated circuit (chip) doubles roughly every two years” Closely linked, but not necessarily related to performance The “power wall” slows performance improvements of single core/single processor systems A few cores for multiple “programs” is easy to justify How to use massively[大规模的] parallel computers/CPUs/many core machines Can we extract parallelism automatically, can we implement parallelism at the lowest level (similar to multiprogramming) Multi-core, hyperthreaded processors Modern CPUs contain multiple cores and are often hyper-threaded Evolution in hardware has implications on operating system design XP did not support multi processor architectures Process scheduling needs to account for load balancing and CPU affinity[亲和性] Cache coherency[缓存一致性] becomes important Memory Memory hierarchies[层级] used to balance cost and performance Fast and expensive memory is used for caching Slow and inexpensive memory is used for long term storage Memory includes, registers, L1/L2 cache, main/core memory, disk, etc. L2 Cache can be shared or dedicated[专注的] to individual cores Cache management is mainly done by hardware The CPU can only access main memory directly (i.e. files have to be brought into memory first) I/O Devices Device driver interacts[交互] with the controller, controller interacts with the device (e.g., disk controller) The operating system/device driver typically communicates with the controller through registers I/O can take place through: Busy waiting Interrupt based Direct memory access (using DMA chip) Operating System Structure Systems contain a lot of functionality Operating Systems are structured by Micro kernels[微内核] and Monolithic[单内核] Micro Kernels All non-essential functionality is extracted[取出] from the kernel Communication, memory management and CPU scheduling are likely to be included in the kernel The file system, GUI, device drivers are likely to be user processes除了保留基本功能，其他功能移出到user mode Micro kernels are more easy to extend, more portable[便携], and usually more reliable Frequent system calls and kernel traps[陷阱] cause significant overhead[开销] (mode switches) Some Unix version, Mac OS X, Minix, and early versions of Windows (NT4.0) were (partially) micro kernels Monolithic Systems All procedures are linked together into one single executable running in kernel mode Monolithic kernels are difficult to maintain Current versions of Windows, Linux are implemented as monolithic kernels操作系统高度紧密，移植性不佳。但是若设计完善，效率高 Summary Operating Systems are closely linked to computer architecture Address translation and interrupts OS structures","tags":[{"name":"笔记","slug":"笔记","permalink":"http://barondu.com/tags/笔记/"},{"name":"OSC","slug":"OSC","permalink":"http://barondu.com/tags/OSC/"},{"name":"Introduction","slug":"Introduction","permalink":"http://barondu.com/tags/Introduction/"}]},{"title":"笔记 OSC_Introduction1","date":"2017-01-01T15:06:04.000Z","path":"2017/01/01/Introduction1/","text":"Introduction 1 Overview “Defining“ operating systems What is multi-programming Kernel-user mode Defining Operating Systems In the early days, programmers had to deal directly with hardware Real computer hardware is urgly Hardware is extremely difficult to program An operating system is a layer[层] of indirection[间接] on top of the hardware: It provide abstractions for application programs it provide a cleaner and easier interface to the hardware Multi-programming Morden OS use multi-programming to improve user experience and maximise the use of resource Disk is slow. CPU is faster than disk. Without multi-programming, CPU time is waste while wating for I/O requests. Multi-programming has important consequences[结果] for operating system design The operating system must allocate[分配]/share resources (CPU, memory, I/O devices) fairly and safely between competing processes: In time, e.g. CPUs and printers In space, e.g., memory and disks The execution of multiple programs (processes) needs to be interleaved[交错] with one another. This requires: This requires context switches and process scheduling ⇒ mutual exclusion[相互排斥], deadlock avoidance, protection, . . . Kernel-user mode Modern operating systems have multiple modes: The operating system runs in kernel mode and has access to all instructions Applications run in user mode and have access to a subset of instructions Transitions from user mode to kernel mode happen in a controlled manner (interrupts, exceptions, system calls) and are mirrored in hardware Summary Some properties: Sits directly on top of the hardware Has access to the full capabilities of the hardware Provides abstractions for the user/programmer Makes sure that everything is organised and runs in order Improve the hardware interface","tags":[{"name":"笔记","slug":"笔记","permalink":"http://barondu.com/tags/笔记/"},{"name":"OSC","slug":"OSC","permalink":"http://barondu.com/tags/OSC/"},{"name":"Introduction","slug":"Introduction","permalink":"http://barondu.com/tags/Introduction/"}]}]