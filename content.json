[{"title":"拓展 N和NP问题","date":"2017-02-06T11:43:27.000Z","path":"2017/02/06/N和NP问题/","text":"N和NP问题 “P” refers to the class of problems that can be solved in polynomial time“NP” refers to problems that can be solved in non- deterministic polynomial time P是能在多项式时间内解决的问题， NP是能在多项式时间验证答案正确与否的问题。 用大白话讲大概就是这样。所以P是否等于NP实质上就是在问，如果对于一个问题我能在多项式时间内验证其答案的正确性，那么我是否能在多项式时间内解决它？这个表述不太严谨，但通俗来讲就是如此。(2014, Wang) ####什么是polynomial time？ 多项式级的复杂度：O(1),O(log(n)),O(n^a)等，因为它的规模n出现在底数的位置； 非多项式级的复杂度：O(a^n)和O(n!)型复杂度，其复杂度计算机往往不能承受。","tags":[{"name":"G52LAC","slug":"G52LAC","permalink":"http://www.barondu.com/tags/G52LAC/"}]},{"title":"笔记 OSC_Concurrency2","date":"2017-01-09T17:33:27.000Z","path":"2017/01/09/Concurrency2/","text":"Concurrency 2 Overview Software approaches: Peterson’s solution Hardware approaches: Disabling interrupts: test_and_set() compare_and_swap() Higher level approaches include mutexes and semaphoresPeterson’s Solution Peterson’s solution is a software based solution which worked well for older machines Two shared variables are used: turn: indicates which process is next to enter its critical section boolean flag[2]: indicates that a process is ready to enter its critical section It is restricted[限制] to two processes that execute in strict alternation[严格的交替]1234567891011do &#123; flag[j] = true; // j wants to enter critical section turn = i; // allow i to access first while (flag[i] &amp;&amp; turn == i); // whilst i wants to access critical section // and its i’s turn, apply busy waiting // CRITICAL SECTION flag[j] = false; // remainder section&#125; while (...); Peterson’s solution satisfies all requirements for mutual exclusion Mutual exclusion requirement: the variable turn can have at most one value at a time while(flag[i] &amp;&amp; turn == i)or while(flag[j] &amp;&amp; turn == j) is true and at most one process can enter its critical section (mutual exclusion) Disabling Interrupts Disable interrupts whilst executing a critical section and prevent interruption (i.e., interrupts from timers, I/O devices, etc.) Think of the counter++ example = counter;12register = register + 1counter = register; Disabling interrupts “may” be appropriate on a single CPU machine This is inefficient on modern multi-core/multi processor machines Disabling interrupts on all cores/CPUs takes time and causes delays CPU capacity[能力] is lost on other cores Atomic Instructions Implement test_and_set() and swap_and_compare() instructions as a set of atomic (UN-interruptible) instructions Reading and setting the variable(s) is done as one “complete” set of instructions If test_and_set() or compare_and_swap() are called simultaneously, they will be executed sequentially They are used in in combination with global lock variables, assumed to be true if the lock is in use test_and_set() Test and set must be atomic/UN-interruptable1234567891011121314151617// Test and set methodboolean test_and_set(boolean * lock) &#123; boolean rv = *lock; *lock = true; return rv;&#125;// Example of using test and set methoddo &#123; // WHILE the lock is in use, apply busy waiting while (test_and_set(&amp;lock)); // Lock was false, now true // CRITICAL SECTION ... lock = false; ... // remainder section&#125; while (...) compare_and_swap()1234567891011121314151617// Compare and swap methodint compare_and_swap(int *lock, int expected, int new_value) &#123; int temp = *lock; if(*lock == expected) *lock = new_value; return temp;&#125;// Example using compare and swap methoddo &#123; // While the lock is in use (i.e. == 1), apply busy waiting while (compare_and_swap(&amp;lock, 0, 1) != 0); // Lock was false, now true // CRITICAL SECTION ... lock = 0; ... // remainder section&#125; while (...); Disadvantages: test_and_set()and compare_and_swap() are hardware instructions and (usually) not directly accessible to the user Busy waiting is used Starvation is possible Deadlock is possible The OS uses the hardware instructions to implement higher level mechanisms/instructions for mutual exclusion, i.e. mutexes and semaphores ##Summary Peterson’s solution (software) Hardware instructions: interrupt disabling, (test_and_set, compare_and_swap)","tags":[{"name":"OSC","slug":"OSC","permalink":"http://www.barondu.com/tags/OSC/"},{"name":"concurrency","slug":"concurrency","permalink":"http://www.barondu.com/tags/concurrency/"}]},{"title":"笔记 OSC_Concurrency1","date":"2017-01-09T14:09:27.000Z","path":"2017/01/09/Concurrency1/","text":"Concurrency 1 Overview Examples of concurrency issues (e.g. counter++) Root causes of concurrency issues Critical sections and mutual exclusion Requirements and approaches for mutual exclusion ###concurrency Threads and processes execute concurrently or in parallel and can share resources Multiprogramming/multiprocessing improves system utilisation A process/thread can be interrupted at any point in time (I/O, timer) The process “state” is saved in the process control block The outcome of programs may become unpredictable[不可预知的] Sharing data can lead to inconsistencies[矛盾] I.e., the outcome of execution may depend on the order win which instructions are carried out Incrementing a counter counter++ consists of three separate actions: 1 read the value of counter and store it in a register 2 add one to the value in the register 3 store the value of the register in counter The above actions are NOT “atomic”, e.g. they can be interrupted by the timer (⇒ context switch) Bounded Buffers Consider a bounded buffer in which N items can be stored A counter is maintained to count the number of items currently in the buffer Incremented when an item is added Decremented when an item is removed Similar concurrency problems as with the calculation of sums happen in the bounded buffer (producer/consumer) problem Race Conditions A race condition occurs when multiple threads/processes access shared data and the result is dependent on the order in which the instructions are interleaved Concurrency within the OSData Structures Kernels are preemptive these days (⇔ non-preemptive) Multiple processes are running in the kernel I.e. kernel processes can be interrupted at any point The kernel maintains data structures, e.g. process tables, memory structures, open file lists, etc. These data structures are accessed concurrently/in parallel These can be subject to concurrency issues Resources Processes share resources, including memory, files, processor time, printers, I/O devices, etc. The operating system must: Allocate and deallocate these resources safely (i.e. avoid interference, deadlocks and starvation) Make sure that interactions within the OS do not result in race conditions The operating system must provide locking mechanisms to implement/support mutual exclusion (and prevent starvation and deadlocks) Critical Sections[临界段], Mutual Exclusion[互斥] A critical section is a set of instructions in which shared variables are changed Mutual exclusion must be enforced for critical sections Only one process at a time should be in the critical section (mutual exclusion) Processes have to get “permission” before entering their critical section123456789do&#123; ... // ENTRY to critical section critical section, e.g.counter++; // EXIT critical section remaining code ...&#125; while (...); Any solution to the critical section problem must satisfy the following requirements: Mutual exclusion: only one process can be in its critical section at any one point in time Progress: any process must be able to enter its critical section at some point in time Fairness/bounded waiting: processes cannot be made to wait indefinitely These requirements have to be satisfied, independent of the order in which sequences are executed Enforcing Mutual Exclusion Approaches for mutual exclusion can be: Software based: Peterson’s solution Hardware based: test_and_set(), swap_and_compare() Based on: Mutexes Semaphores Monitors (software construct within the programming languages) In addition to mutual exclusion, deadlocks have to be prevented (next week’s subject) Summary Examples of concurrency issues (e.g. counter++) Root causes of concurrency issues Critical sections and mutual exclusion Requirements and approaches for mutual exclusion","tags":[{"name":"OSC","slug":"OSC","permalink":"http://www.barondu.com/tags/OSC/"},{"name":"concurrency","slug":"concurrency","permalink":"http://www.barondu.com/tags/concurrency/"}]},{"title":"笔记 OSC_Processes4","date":"2017-01-09T14:05:27.000Z","path":"2017/01/09/Processes4/","text":"Processes 4 Overview Multi-level feedback queues Scheduling in Window 7 + illustration Scheduling in Linux (implementation in labs) Scheduling related processes/threads Multi-level Feedback Queues Different scheduling algorithms can be used for the individual queues (e.g., round robin, SJF, FCFS) Feedback queues allow priorities to change dynamically[动态的], i.e., jobs can move between queues: Move to lower priority queue if too much CPU time is used (prioritise I/O and interactive processes) Move to higher priority queue to prevent starvation and avoid inversion[反转] of control Defining characteristics of feedback queues include: The number of queues The scheduling algorithms used for the individual queues Migration policy[迁移政策] between queues Initial access to the queuesFeedback queues are highly configurable[可配置] and offer significant flexibility[灵活性，适应性] Windows 7 An interactive system[交互系统] using a preemptive scheduler with dynamic priority levels Two priority classes with 16 different priority levels exist “Real time” processes/threads have a fixed priority level “Variable” processes/threads can have their priorities boosted temporarily A round robin algorithm is used within the queues Priorities are based on the process base priority (between 0-15) and thread base priority (±2 relative to the process priority) A thread’s priority dynamically changes during execution between its base priority and the maximum priority within its class Interactive I/O bound processes (e.g. keyboard) receive a larger boost Boosting priorities prevents priority inversion Scheduling in LinuxThe Completely Fair Scheduler Linux distinguishes[区分] between two types of tasks for scheduling: Real time tasks (to be POSIX compliant), divided into: Real time FIFO tasks Real time Round Robin tasks Time sharing tasks using a preemptive approach (similar to variable in Windows) The most recent scheduling algorithm in Linux for time sharing tasks is the “completely fair scheduler” Real-Time Tasks Real time FIFO tasks have the highest priority and are scheduled using a FCFS approach, using preemption if a higher priority job shows up Real time round robin tasks are preemptable by clock interrupts and have a time slice associated with them Both approaches cannot guarantee hard deadlines Time Sharing Tasks The CFS divides the CPU time between all processes If all N processes have the same priority: They will be allocated a “time slice” equal to 1/N times the available CPU time I.e., if N equals 5, every process will receive 20% of the processor’s time The length of the time slice and the “available CPU time” are based on the targeted latency[延迟] (⇒ every process should run at least once during this interval) If N is very large, the context switch time will be dominant, hence a lower bound on the “time slice” is imposed by the minimum granularity[粒度] A process’s time slice can be no less than the minimum granularity (response time will deteriorate[恶化]) A weighting scheme is used to take different priorities into account If process have different priorities: Every process i is allocated a weight Wi that reflects its priority The tasks with the lowest amount of “used CPU time” are selected first Shared Queues A single or multi-level queue shared between all CPUs Advantage: automatic load balancingDisadvantages: Contention[竞争] for the queues (locking is needed) “All CPUs are equal, but some are more equal than others” : does not account for processor affinity[紧密度]: Cache becomes invalid when moving to a different CPU Translation look aside buffers (TLBs - part of the MMU) become invalid Windows will allocate the highest priority threads to the individual CPUs/cores Private Queues Each process/thread is assigned to a queue private to an individual CPU Advantages: CPU affinity is automatically satisfied Contention for shared queue is minimised Disadvantages: less load balancing Push and pull migration between CPUs is possible Related vs. Unrelated ThreadsRelated: multiple threads that communicate with one another and ideally run together (e.g. search algorithm) Unrelated: e.g. processes threads that are independent, possibly started by different users running different programs ####Related Threads The aim is to get threads running, as much as possible, at the same time across multiple CPUs Approaches include: Space sharing Gang scheduling ####Gang scheduling Time slices are synchronised[同步的] and the scheduler groups threads together to run simultaneously (as much as possible) A preemptive algorithm Blocking threads result in idle CPUs ##Summary Scheduling on Windows and Linux Multi-processor/core scheduling is “a bit different” (load balancing, processor affinity, etc.) Related and unrelated threads Shared or private queues Space scheduling or gang scheduling","tags":[{"name":"OSC","slug":"OSC","permalink":"http://www.barondu.com/tags/OSC/"},{"name":"processes","slug":"processes","permalink":"http://www.barondu.com/tags/processes/"}]},{"title":"笔记 OSC_Processes3","date":"2017-01-08T23:27:27.000Z","path":"2017/01/08/Processes3/","text":"Processes 3 Overview1 Threads vs. processes2 Different thread implementations3 POSIX Threads (PThreads) ThreadsThreads from an OS Perspective[观点] A process consists of two fundamental units Resources: all related resources are grouped together A logical address space containing the process image (program, data, heap, stack) Files, I/O devices, I/O channels, . . . Execution trace[执行追踪], i.e., an entity that gets executed A process can share its resources between multiple execution traces, i.e., multiple threads running in the same resource environment Every thread has its own execution context (e.g. program counter, stack, registers) All threads have access to the process’ shared resources E.g. files, one thread opens a file, all threads of the same process can access the file Global variables, memory, etc. (⇒ synchronisation!) Some CPUs (hyperthreaded ones) have direct hardware support for multi-threading Similar to processes, threads have: States and transitions (new, running, blocked, ready, terminated) A thread control blockThreads create/terminate/switch with less overhead (address space remains the same for threads of the same process) Inter-thread communication is easier/faster than inter-process communication (threads share memory by default) No protection boundaries[边界] are required in the address space (threads are cooperating, belong to the same user, and have a common goal) Synchronisation has to be considered carefully! Why Use Threads Multiple related activities apply to the same resources, these resources should be accessible/shared Processes will often contain multiple blocking tasks I/O operations (thread blocks, interrupt marks completion) Memory access: pages faults are result in blocking Such activities should be carried out in parallel/concurrently Application examples: webservers, make program, spreadsheets, word processors, processing large data volumes ####OS Implementations of Threads User threads Kernel threads Hybrid[混合] implementations User ThreadsMany-to-One Thread management (creating, destroying, scheduling, thread control block manipulation[处理]) is carried out in user space with the help of a user library The process maintains a thread table managed by the runtime system without the kernel’s knowledge Similar to process table Used for thread switching Tracks thread related information Advantages: Threads are in user space (i.e., no mode switches required) Full control over the thread scheduler OS independent[独立不受约束] (threads can run on OS that do not support them) Disadvantages: Blocking system calls suspend[延缓] the entire process (user threads are mapped onto a single process, managed by the kernel) No true parallelism (a process is scheduled on a single CPU) Clock interrupts are non-existent (i.e. user threads are non-preemptive) Page faults[错误] result in blocking the process Kernel ThreadsOne-to-One The kernel manages the threads, user application accesses threading facilities[工具] through API and system calls Thread table is in the kernel, containing thread control blocks (subset of process control blocks) If a thread blocks, the kernel chooses thread from same or different process (↔ user threads) Windows and Linux apply this approach Advantages: True parallelism can be achieved No run-time system needed Disadvantage: Frequent mode switches take place, resulting in lower performance Frequent mode switches take place, resulting in lower performance Performance Hybrid ImplementationsMany-to-Many User threads are multiplexed onto kernel threads Kernel sees and schedules the kernel threads (a limited number) User application sees user threads and creates/schedules these (an “unrestricted” number) Comparison Thread Management Thread libraries provide an API/interface for managing threads (e.g. creating, running, destroying, synchronising, etc.) Thread libraries can be implemented: Entirely in user space (i.e. user threads) Based on system calls Examples of thread APIs include POSIX’s PThreads, Windows Threads, and Java Threads The PThread specification can be implemented as user or kernel threads POSIX threads are a specification that “anyone” can implement, i.e., it defines a set of APIs (function calls, over 60 of them) and what they do Summary Threads vs. processes Thread implementations (user, kernel and hybrid) PThreads","tags":[{"name":"OSC","slug":"OSC","permalink":"http://www.barondu.com/tags/OSC/"},{"name":"processes","slug":"processes","permalink":"http://www.barondu.com/tags/processes/"}]},{"title":"笔记 OSC_Processes2","date":"2017-01-08T23:26:14.000Z","path":"2017/01/08/Processes2/","text":"Processes 2 Overview Introduction to process scheduling Types of process schedulers Evaluation criteria for scheduling algorithms Typical process scheduling algorithms Process SchedulingContext The OS is responsible for managing and scheduling processes Decide when to admit processes to the system (new → ready) Decide which process to run next (ready → run) Decide when and which processes to interrupt (running → ready) It relies on the scheduler (dispatcher) to decide which process to run next, which uses a scheduling algorithm to do so The type of algorithm used by the scheduler is influenced by the type of operating system (e.g., real time vs. batch) Classification by Time Horizon Long term: applies to new processes and controls the degree of multiprogramming by deciding which processes to admit to the system A good mix of CPU and I/O bound processes is favourable to keep all resources as busy as possible Usually absent in popular modern OS Medium term: controls swapping and the degree of multi-programming Short term: decide which process to run next Usually called in response to clock interrupts, I/O interrupts, or blocking system calls Invoked[调用] very frequently, hence must be fast Manages the ready queue Classification by Approach Non-preemptive: processes are only interrupted voluntarily[自愿的] (e.g., I/O operation or “nice” system call – yield()) Preemptive[优先的]: processes can be interrupted forcefully or voluntarily This requires context switches which generate overhead, too many of them should be avoidedPrevents processes from monopolising[垄断的] the CPU Most popular modern operating systems are preemptive Performance Assessment[性能评估]User oriented[导向]criteria: Response time: minimise the time between creating the job and its first execution Turnaround time: minimise the time between creating the job and finishing it Predictability[可调度性]: minimise the variance[差异，方差] in processing times System oriented criteria: Throughput[吞吐量]: maximise the number of jobs processed per hour Fairness[公平性]: Are processing power/waiting time equally distributed? Are some processes kept waiting excessively long (starvation) Evaluation criteria can be conflicting, i.e., reducing the response time may increase context switches and may worsen the throughput and increase the turn around time Scheduling AlgorithmsAlgorithms considered: First Come First Served (FCFS)/ First In First Out (FIFO) Shortest job first Round Robin Priority queues Performance measures used: Average response time Average turnaround time First Come First Served Concept: a non-preemtive algorithm that operates as a strict[严格的] queueing mechanism[机制] and schedules the processes in the same order that they were added to the queue Advantages: positional fairness and easy to implement Disadvantages: good for long processes over short ones Could compromise[危害] resource utilisation, i.e., CPU vs. I/O devices Shortest Job First Concept: A non-preemtive algorithm that starts processes in order of ascending[递增] processing time using a provided/known estimate of the processing Advantages: always result in the good turn around time Disadvantages: Starvation[饿死] might occur Fairness and predictability are compromised Processing times have to be known beforehand Round Robin Concept: a preemptive version of FCFS that forces context switches at periodic[周期性] intervals[间隔] or time slices Processes run in the order that they were added to the queue Processes are forcefully interrupted by the timerAdvantages: Improved response time Effective for general purpose time sharing systemsDisadvantages: Increased context switching and thus overhead Favours CPU bound processes (which usually run long) over I/O processes (which do not run long) Can reduce to FCFS The length of the time slice must be carefully considered! a low response time is achieved with a small time slice (e.g. 1ms) ⇒ low throughput a high throughput is achieved with a large time slice (e.g. 1000ms) ⇒ high response time If a time slice is only used partially, the next process starts immediately Priority Queues Concept: A preemptive algorithm that schedules processes by priority (high → low) The process priority is saved in the process control block Advantages: can prioritise[优先] I/O bound jobs Disadvantages: low priority processes may suffer from starvation (with static priorities) ##Summary Types of schedulers: preemptive/non-preemptive, long/medium/short term) Performance evaluation criteria Scheduling algorithms: FCFS, SJF, Round Robin, Priority Queues","tags":[{"name":"OSC","slug":"OSC","permalink":"http://www.barondu.com/tags/OSC/"},{"name":"processes","slug":"processes","permalink":"http://www.barondu.com/tags/processes/"}]},{"title":"笔记 OSC_Processes1","date":"2017-01-06T11:42:14.000Z","path":"2017/01/06/Processes1/","text":"Processes 1 Overview Introduction to processes and their implementation Process states and state transitions System calls for process management Processes and Implementation Definition: “a process is a running instance of a program”进程是程序的运行实例 A process is registered with the OS using its “control structures”: i.e. an entry in the OS’s process table to a process control blocks (PCB) The process control block contains all information necessary to manage the process and is necessary for context switching in multi-programmed systems A process’ memory image contains: The program code (could be shared between multiple processes running the same code) A data segment, stack and heap Every process has its own logical address space, in which the stack and heap are placed at opposite[相反的] sides to allow them to grow Process States and TransitionsSates: A new process has just been created (has a PCB) and is waiting to be admitted (it may not yet be in memory) A ready process is waiting for CPU to become available (e.g. unblocked or timer interrupt) A running process “owns” the CPU A blocked process cannot continue, e.g. is waiting for I/O A terminated process is no longer executable (the data structures - PCB - may be temporarily preserved) A suspended[废除的] process is swapped out[换出] (not discussed further) Transitions： New -&gt; ready: admit the process and commit to execution Running -&gt; blocked: e.g. process is waiting for input or carried out asystem call Ready -&gt; running: the process is selected by the process scheduler Blocked -&gt; ready: event happens, e.g. I/O operation has finished . Running -&gt; ready: the process is preempted, e.g., by a timer interrupt orby pause Running -&gt;strong text exit: process has finished, e.g. program ended or exceptionencountered The interrupts/traps/system calls lie on the basis of the transitions Context SwitchingMulti-programming Modern computers are multi-programming systems Assuming a single processor system, the instructions of individual processes are executed sequentially Multi-programming goes back to the “MULTICS” age Multi-programming is achieved by alternating[交替]processes and context switching True parallelism requires multiple processors并不是真正的multi-programming When a context switch takes place, the system saves the state of the old process and loads the state of the new process (creates overhead) Saved -&gt; the process control block is updated (Re-)started -&gt; the process control block read A trade-off[权衡] exists between the length of the time-slice and the context switch time Short time slices result in good response times but low effective “utilisation”[使用] e.g.: 99*(1+1)=198ms Long time slices result in poor response times but better effective “utilisation” e.g.: 99 * (100 + 1) = 9999ms A process control block contains three types of attributes: Process identification (PID, UID, Parent PID) Process control information (process state, scheduling information, etc.) Process state information (user registers, program counter, stack pointer, program status word, memory management information, files, etc.) Process control blocks are kernel data structures, i.e. they are protected and only accessible in kernel mode! Allowing user applications to access them directly could compromise[威胁] their integrity[完整性] The operating system manages them on the user’s behalf through system calls (e.g. to set process priority) Tables and Control Blocks An operating system maintains information about the status of “resources” in tables Process tables (process control blocks) Memory tables (memory allocation, memory protection, virtual memory) I/O tables (availability, status, transfer information) File tables (location, status) The process table holds a process control block for each process, allocated upon process creation Tables are maintained by the kernel and are usually cross referencedSwitching Processes Save process state (program counter, registers) Update PCB (running -&gt; ready/blocked) Move PCB to appropriate queue (ready/blocked) Run scheduler, select new process Update to running state in the new PCB Update memory management unit (MMU) Restore process System Calls System calls are necessary to notify the OS that the process has terminated Resources must be de-allocated Output must be flushed Process admin may have to be carried out A system calls for process termination: UNIX/Linux: exit(), kill() Windows: TerminateProcess() Summary Definition of a process and their implementation in operating systems States, state transitions of processes Kernel structures for processes and process management System calls for process management","tags":[{"name":"OSC","slug":"OSC","permalink":"http://www.barondu.com/tags/OSC/"},{"name":"processes","slug":"processes","permalink":"http://www.barondu.com/tags/processes/"}]},{"title":"笔记 OSC_Introduction2","date":"2017-01-03T23:50:14.000Z","path":"2017/01/03/Introduction2/","text":"Introduction 2 Overview CPU design Address spaces, interruts OS structures/implementation CPU design A CPU basic cycle consist of fetch[取], decode, execute Every CPU has his own instruction set CPU has a set of registers Registers are used to store data and for special functions (e.g. program counter, program status word – mode bit) The compiler/programmer decides what to keep in the registers Context switching[上下文切换] must save and restore the CPU’s internal state, including its registers Memory Management Unit (MMU) Memory adsresses from 0 to MAX Variables are mnemonic[帮助记忆的] names for memory addresses You don’t know where the process will run in physical memory at compile time Multiple processes run on modern machines The compiler assumes that it will start running at 0 (logical address space) An offset[补偿] is added at runtime by the MMU (physical address space) physical address = logical address + offset Modern computer use a logical and physical memory addresses: Every process has a logical address space – [0,MAX64] (theoretically理论上) The machine has a physical address space – [0, MAX ] (MAX determined by the amount of physical memory) Address translation takes place in MMU physical = f (logical ) A context switch between processes invalidates[使无效] the MMU (as well as registers, cache, … ) Timer interrupts Interrupts temporarily pause a process’s normal operation Different types of interrupts: Timer interrupts by CPU clock I/O interrupts for I/O completion or error codes Software generated, e.g. errors and exceptions Timer generates an interrupt CPU finishes current instruction and tests for interrupt Transfer to interrupt service routine Hardware saves current process state (PSW, program counter) Set program counter to interrupt service routine Save registers and other state information Carry out[执行] interrupt service routine (scheduler) Restore next process to run Moore’s “law” Moore’s “law”: “The number of transistors on an integrated circuit (chip) doubles roughly every two years” Closely linked, but not necessarily related to performance The “power wall” slows performance improvements of single core/single processor systems A few cores for multiple “programs” is easy to justify How to use massively[大规模的] parallel computers/CPUs/many core machines Can we extract parallelism automatically, can we implement parallelism at the lowest level (similar to multiprogramming) Multi-core, hyperthreaded processors Modern CPUs contain multiple cores and are often hyper-threaded Evolution in hardware has implications on operating system design XP did not support multi processor architectures Process scheduling needs to account for load balancing and CPU affinity[亲和性] Cache coherency[缓存一致性] becomes important Memory Memory hierarchies[层级] used to balance cost and performance Fast and expensive memory is used for caching Slow and inexpensive memory is used for long term storage Memory includes, registers, L1/L2 cache, main/core memory, disk, etc. L2 Cache can be shared or dedicated[专注的] to individual cores Cache management is mainly done by hardware The CPU can only access main memory directly (i.e. files have to be brought into memory first) I/O Devices Device driver interacts[交互] with the controller, controller interacts with the device (e.g., disk controller) The operating system/device driver typically communicates with the controller through registers I/O can take place through: Busy waiting Interrupt based Direct memory access (using DMA chip) Operating System Structure Systems contain a lot of functionality Operating Systems are structured by Micro kernels[微内核] and Monolithic[单内核] Micro Kernels All non-essential functionality is extracted[取出] from the kernel Communication, memory management and CPU scheduling are likely to be included in the kernel The file system, GUI, device drivers are likely to be user processes除了保留基本功能，其他功能移出到user mode Micro kernels are more easy to extend, more portable[便携], and usually more reliable Frequent system calls and kernel traps[陷阱] cause significant overhead[开销] (mode switches) Some Unix version, Mac OS X, Minix, and early versions of Windows (NT4.0) were (partially) micro kernels Monolithic Systems All procedures are linked together into one single executable running in kernel mode Monolithic kernels are difficult to maintain Current versions of Windows, Linux are implemented as monolithic kernels操作系统高度紧密，移植性不佳。但是若设计完善，效率高 Summary Operating Systems are closely linked to computer architecture Address translation and interrupts OS structures","tags":[{"name":"OSC","slug":"OSC","permalink":"http://www.barondu.com/tags/OSC/"},{"name":"Introduction","slug":"Introduction","permalink":"http://www.barondu.com/tags/Introduction/"}]},{"title":"笔记 OSC_Introduction1","date":"2017-01-01T15:06:04.000Z","path":"2017/01/01/Introduction1/","text":"Introduction 1 Overview “Defining“ operating systems What is multi-programming Kernel-user mode Defining Operating Systems In the early days, programmers had to deal directly with hardware Real computer hardware is urgly Hardware is extremely difficult to program An operating system is a layer[层] of indirection[间接] on top of the hardware: It provide abstractions for application programs it provide a cleaner and easier interface to the hardware Multi-programming Morden OS use multi-programming to improve user experience and maximise the use of resource Disk is slow. CPU is faster than disk. Without multi-programming, CPU time is waste while wating for I/O requests. Multi-programming has important consequences[结果] for operating system design The operating system must allocate[分配]/share resources (CPU, memory, I/O devices) fairly and safely between competing processes: In time, e.g. CPUs and printers In space, e.g., memory and disks The execution of multiple programs (processes) needs to be interleaved[交错] with one another. This requires: This requires context switches and process scheduling ⇒ mutual exclusion[相互排斥], deadlock avoidance, protection, . . . Kernel-user mode Modern operating systems have multiple modes: The operating system runs in kernel mode and has access to all instructions Applications run in user mode and have access to a subset of instructions Transitions from user mode to kernel mode happen in a controlled manner (interrupts, exceptions, system calls) and are mirrored in hardware Summary Some properties: Sits directly on top of the hardware Has access to the full capabilities of the hardware Provides abstractions for the user/programmer Makes sure that everything is organised and runs in order Improve the hardware interface","tags":[{"name":"OSC","slug":"OSC","permalink":"http://www.barondu.com/tags/OSC/"},{"name":"Introduction","slug":"Introduction","permalink":"http://www.barondu.com/tags/Introduction/"}]}]